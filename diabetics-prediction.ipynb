{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72976822",
   "metadata": {},
   "source": [
    "# Hands On Workshop - Big Data in Healthcare 8400\n",
    "### Hadas Volkov\n",
    "## Predicting Diabetes - A Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2c84",
   "metadata": {},
   "source": [
    "In 2018, about 10.5% of Americans were estimated to have diabetes. Furthermore, about one-fifth of those cases were undiagnosed. Early detection is key in diabetes because early treatment can prevent serious complications. When a problem with blood sugar is found, doctors and patients can take steps to prevent permanent damage to the heart, kidneys, eyes, nerves, blood vessels, and other vital organs. </br>\n",
    "A patient must go through several tests, and checked for multiple factors, in order to be diagnosed with diabetes. The long process makes it difficult for doctors to keep track and can lead to inaccurate results which makes the detection very challenging. Due to recent advances in machine learning algorithms it is now possible to conduct a fast and accurate prediction of the disease in candidate patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041e19d",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. [Pima Indians Diabetes Database on Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46124a-b073-42d1-94b3-25321389fc29",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "The platform chosen for this exercise is a Jupyter notebook. For data scientists, notebooks are a crucial tool. Notebooks are a form of interactive computing, in which users write and execute code, visualize the results, and share insights. Typically, data scientists use notebooks for experiments and exploration tasks. </br>\n",
    "It is not expected of you to fully understand the code, it is here for you if you’d like to dive deeper, but whether this form of presentation allows me to integrate the computing environment and to facilitate the work of a data scientist to you. </br>\n",
    "You are asked to follow the notebook and execute each code block by highlighting the block and using the ‘play’ button above, or use the keyboard shortcut ‘shift+enter’ to execute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120f201-c826-4ccf-a461-b9c1cc6a7af8",
   "metadata": {},
   "source": [
    "#### Python and python packages\n",
    "The python programing language has dominated the field of machine learning for the past years. There are two main reasons; One, python is a relatively simple to pick up for non-coders and facilitate the most intuitive programing syntax. The second reason, and the more important one, is the abundance of packages available for python users, especially for data scientists. A python package is a program written in python and offers some specific functionality to the user. For example, the ‘pandas’ package allows for handling csv and text files easily, ‘scikit-learn’ wraps almost all common machine learning algorithms, making it easy for us to quickly test variety of methods on our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7c9c5-0acb-45b7-97a1-51b0e1360d38",
   "metadata": {},
   "source": [
    "In the bellow code block we’ll import some packages and functions for our analysis. Please execute the block before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc5dfee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4805bdd-fcc8-4d7e-867f-77a13d4ebbfa",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c7570-ce09-4245-be5a-01d2508bc555",
   "metadata": {},
   "source": [
    "We will start by importing our dataset to our environment and saving it under the name ‘df’ (short for csv dataframe). The data is kept in a ‘csv’ file named ‘diabetes.csv’ in the current directory as this notebook. </br>\n",
    "The command ‘df.head()’ will print the first five rows in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc735f3-aaac-452f-93c4-554cb2399123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "df = pd.read_csv('diabetes.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af17b4-a850-4e9f-9f29-3588d9f848b9",
   "metadata": {},
   "source": [
    "The dataset contains 768 observable values with eight feature variables and one target variable. Before starting to analyze the data and draw any conclusions, it is essential to understand the presence of missing values in any dataset. To do so, the simplest way is to use 'df.info()' function which will provide us the column names with the type of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60115f86-de7a-41c8-8522-f58ace5d5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information on the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3143c-7229-418c-b5b0-f84e0a7ab835",
   "metadata": {},
   "source": [
    "There are five features in the data that contain null values, Glucose, BloodPressure, SkinThickness, Insulin and BMI. A null value is a special marker to indicate that a data value does not exist in the database. In other words, it is just a placeholder to denote values that are missing or that we do not know. </br> \n",
    "We can perform a quick calculation and print the percentage of missing values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989c735-2799-4b18-87af-43153a11dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of columns with total number of missing values\n",
    "print('Column'+ '\\t\\t\\t\\t Total missing Values'+'\\t\\t\\t\\t % of missing values')\n",
    "#print(\"\\n\")\n",
    "for i in df.columns:\n",
    "    print(f\"{i: <50}{df[i].isnull().sum():<30}{((df[i].isnull().sum())*100)/df.shape[0]: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92549ad4-963b-4553-bf9c-4b5be48813fd",
   "metadata": {},
   "source": [
    "Next, we can try and discard rows containing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c9313-aeb5-432d-972c-3d10d3781233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows containing a null values and get information on the new dataset\n",
    "df.dropna().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27667fce-e856-4147-ace8-efc72969f0d1",
   "metadata": {},
   "source": [
    "Removing all rows containing null values significantly reduced our dataset to only 392 entries. Instead, ww can perform mean imputation, or mean substitution, replacing missing values of a certain variable by the mean of non-missing cases of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e784d33-0a4c-43cd-b2c4-315dea77d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation on null containing columns\n",
    "for col in ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d20932-0e5c-40aa-bc02-b9b59a78cffe",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb441b0-2c4f-4eb7-a99a-9bd880f27d09",
   "metadata": {},
   "source": [
    "To get an initial ‘feel’ of data we can plot a few visualization schemes for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2765f1-0144-4bd7-a348-f46df1da49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "plt.figure(figsize=(18, 6), dpi=80)\n",
    "sns.boxplot(data=df, orient=\"h\",\n",
    "            palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81e26e-828e-45ae-a3c7-d37029cbecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap correlation\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181a736-864d-423c-80c8-d27d764163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise relationships\n",
    "# A function to compute the correlation coefficient\n",
    "graph = sns.PairGrid(df, hue ='Outcome')\n",
    "# Type of graph for diagonal\n",
    "graph = graph.map_diag(plt.hist)\n",
    "# Type of graph for non-diagonal\n",
    "graph = graph.map_offdiag(plt.scatter)\n",
    "graph = graph.add_legend()\n",
    "\n",
    "#This might take a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e9a81-1a49-464f-80cb-2e2d8795fce6",
   "metadata": {},
   "source": [
    "Any intersting relationships in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb6d86-df53-48cf-bea3-4a91d81b2f1d",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146debeb-b941-4223-94bb-e5e9d20f0535",
   "metadata": {},
   "source": [
    "To proceed with the classification training, we need to separate our dataset to features and targets. By convention features in the dataset are denoted with an ‘X’ and target variables with ‘y’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cff7b-93ab-4e17-ad3a-da562f59538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']\n",
    "sns.countplot(x=\"Outcome\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90455c73-cdbc-4170-b463-23e85d94fedb",
   "metadata": {},
   "source": [
    "Next, we will split the dataset into training and testing groups. We will follow common practice and split the dataset into 80% for the training group and 20% as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4067e6-8610-4639-8cb3-e50786e77b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the dataset to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c636d-2980-48f8-a4a5-40748d250c01",
   "metadata": {},
   "source": [
    "Before implementing classification algorithm, we need to scale the feature variables of our dataset. We will resize the distribution of values for each feature so that the mean of the observed values is 0 and the standard deviation is 1. </br>\n",
    "Why is this scaling important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b890-ccd0-4840-afe1-5f3e08241483",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_x = StandardScaler()\n",
    "X_train = scaling_x.fit_transform(X_train)\n",
    "X_test = scaling_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349a7c-4f6f-4d47-adfc-8848bfb3919b",
   "metadata": {},
   "source": [
    "### Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165867d3-c6eb-455f-b401-c31d9d50704f",
   "metadata": {},
   "source": [
    "In the main part of this workshop, we will experiment with a few common prediction algorithms and asses their performance on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b201d-232e-46ee-a928-96307faa2fac",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7e519-20ec-45fb-955e-72dbe8d6beb7",
   "metadata": {},
   "source": [
    "KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. </br>\n",
    "In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7b389-ea06-4c22-b36d-2fd0463b9ea5",
   "metadata": {},
   "source": [
    "![title](img/knn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebbbf1-b1c8-4209-ae54-e6efd4352f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with one neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Calssifiction accuracy with one neighbor: \", \"{:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd5953-2a39-4646-822d-b0fd49bec5e1",
   "metadata": {},
   "source": [
    "With only one neighbor our model’s classification accuracy is 75.3%. </br>\n",
    "Let’s scan a range of neighbor values to see whether it is possible to better this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a634a-fe48-4f79-9279-7dee4e79e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error1= []\n",
    "error2= []\n",
    "neighbors = range(1,50)\n",
    "for k in neighbors:\n",
    "    # using KNN algorithm\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred1 = knn.predict(X_train)\n",
    "\n",
    "    # stroring the errors\n",
    "    error1.append(np.mean(y_train!= y_pred1))\n",
    "    y_pred2 = knn.predict(X_test)\n",
    "    error2.append(np.mean(y_test != y_pred2))\n",
    "\n",
    "# ploting the graphs for testing and training \n",
    "plt.plot(neighbors, error1, label=\"train\")\n",
    "plt.plot(neighbors, error2, label=\"test\")\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "\n",
    "min_knn_value = min(error2)\n",
    "min_knn_index = error2.index(min_knn_value)\n",
    "print(\"Minimum error with {} neighbors: {:.3f}\".format(max_knn_index, max_knn_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7928e4-e052-47ea-a94d-289ee2ff060b",
   "metadata": {},
   "source": [
    "Minimum test model error is received with 31 neighbors. We can compute accuracy value for that k. Also, lets compute the confusion matrix to find out the precision of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855afce-935a-4cc5-847b-ca0041f9a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=31)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "\n",
    "# Classifiction report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba3218-7680-4952-acb0-0ac69b966542",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df12703-d000-4321-942a-a02c9894d2d1",
   "metadata": {},
   "source": [
    "Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76211ec9-f619-408c-900d-c4f035bb7bf2",
   "metadata": {},
   "source": [
    "![title](img/svm.webp.crdownload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78795b22-c237-4966-a5ab-26f142525916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a svm Classifier\n",
    "clf = SVC(kernel='linear') # Linear Kernel\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "\n",
    "# Classifiction report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f261bbf-787d-4373-932b-49a880f7c23e",
   "metadata": {},
   "source": [
    "#### Random Forests Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2fc61-d02e-4832-bc86-45e191f9b004",
   "metadata": {},
   "source": [
    "Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd3305-a6a5-4d0a-89a9-b822368a3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "\n",
    "# Classifiction report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccb996-e5d1-4a77-9d66-93ce76989d8e",
   "metadata": {},
   "source": [
    "Random forests also offers a good feature selection indicator. The below compution plots the relative importance or contribution of each feature in the prediction. It automatically computes the relevance score of each feature in the training phase. Then it scales the relevance down so that the sum of all scores is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c5de3-c886-45e6-84b9-0300f603de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "feature_imp = pd.Series(rfc.feature_importances_,index=df.columns[:-1]).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f85f2c-3967-4cdd-922e-8054702f3ced",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b568c13-8462-4c31-ba61-f37df01044a0",
   "metadata": {},
   "source": [
    "* Which one of the prediction algorithms presented here is the most preferable for the task?\n",
    "* Can you think on other methods to achieve similar or better performance?\n",
    "* What other measurements can we compute to compare these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29fc27-18f5-4c9a-85c0-930d84a2b201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218de3a4-ed62-43f5-8ab9-6ac0573bc812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c9e5a-aaaf-430c-b1a5-65926dda4aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
