{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72976822",
   "metadata": {},
   "source": [
    "# Hands-On Workshop - Big Data in Healthcare 8400\n",
    "\n",
    "##### A static version of this file can be found in the GitHub repo: [https://github.com/hadasvolk/8400ML-WS](https://github.com/hadasvolk/8400ML-WS)\n",
    "\n",
    "### Hadas Volkov - March 2024\n",
    "\n",
    "Azure space and resources were kindly contributed by **Microsoft**\n",
    "\n",
    "Welcome to this hands-on workshop on Big Data in Healthcare. In this workshop, we will explore the use of machine learning for predicting diabetes. We will be using a dataset that contains various health metrics for a number of individuals, some of whom have been diagnosed with diabetes. Our goal is to train a machine learning model on this data, so that it can predict whether a new individual has diabetes based on their health metrics.\n",
    "\n",
    "We will be covering the following topics:\n",
    "\n",
    "1. Introduction to the dataset and problem\n",
    "2. Data preprocessing and exploration\n",
    "3. Training machine learning models\n",
    "4. Evaluating model performance\n",
    "5. Discussion and next steps\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6466fb2",
   "metadata": {},
   "source": [
    "## Predicting Diabetes - A Machine Learning Approach\n",
    "In this notebook, we will be exploring the use of Machine Learning (ML) for predicting diabetes. Diabetes is a chronic disease that occurs either when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces. Insulin is a hormone that regulates blood sugar. Hyperglycaemia, or raised blood sugar, is a common effect of uncontrolled diabetes and over time leads to serious damage to many of the body's systems, especially the nerves and blood vessels.\n",
    "\n",
    "We will be using a dataset that contains various health metrics for a number of individuals, some of whom have been diagnosed with diabetes. Our goal is to train an ML model on this data, so that it can predict whether a new individual has diabetes based on their health metrics.\n",
    "\n",
    "This is a classic example of a binary classification problem, where each individual is classified into one of two categories: 'has diabetes' or 'does not have diabetes'. We will be exploring various ML models and techniques to tackle this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6041e19d",
   "metadata": {},
   "source": [
    "## 1. Introduction to the dataset and problem\n",
    "### About the Dataset\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. [Pima Indians Diabetes Database on Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c46124a-b073-42d1-94b3-25321389fc29",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "The platform chosen for this exercise is a Jupyter notebook. For data scientists, notebooks are a crucial tool. Notebooks are a form of interactive computing, in which users write and execute code, visualize the results, and share insights. Typically, data scientists use notebooks for experiments and exploration tasks. </br>\n",
    "It is not expected of you to fully understand the code, it is here for you if you’d like to dive deeper, but whether this form of presentation allows me to integrate the computing environment and to facilitate the work of a data scientist to you. </br>\n",
    "You are asked to follow the notebook and execute each code block by highlighting the block and using the ‘play’ button above, or use the keyboard shortcut ‘shift+enter’ to execute.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3120f201-c826-4ccf-a461-b9c1cc6a7af8",
   "metadata": {},
   "source": [
    "#### Python and python packages\n",
    "The python programing language has dominated the field of machine learning for the past years. There are two main reasons; One, python is a relatively simple to pick up for non-coders and facilitate the most intuitive programing syntax. The second reason, and the more important one, is the abundance of packages available for python users, especially for data scientists. A python package is a program written in python and offers some specific functionality to the user. For example, the ‘pandas’ package allows for handling csv and text files easily, ‘scikit-learn’ wraps almost all common machine learning algorithms, making it easy for us to quickly test variety of methods on our data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecc7c9c5-0acb-45b7-97a1-51b0e1360d38",
   "metadata": {},
   "source": [
    "In the bellow code block we’ll import some packages and functions for our analysis. Please execute the block before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5dfee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os for file handling\n",
    "import os\n",
    "\n",
    "# pandas for data manipulation, numpy for numerical computation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and seaborn for plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn for machine learning algorithms and model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# mlxtend for some additional machine learning tools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# tensorflow and keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# scipy for scientific computation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# warnings for handling future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4805bdd-fcc8-4d7e-867f-77a13d4ebbfa",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406c7570-ce09-4245-be5a-01d2508bc555",
   "metadata": {},
   "source": [
    "We will start by importing our dataset to our environment and saving it under the name ‘df’ (short for csv dataframe). The data is kept in a ‘csv’ file named ‘diabetes.csv’ in the current directory as this notebook. </br>\n",
    "The command ‘df.head()’ will print the first five rows in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc735f3-aaac-452f-93c4-554cb2399123",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dataset and save it as 'df'\n",
    "df = pd.read_csv('diabetes.csv') \n",
    "\n",
    "# Display the first five rows in the file\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1af17b4-a850-4e9f-9f29-3588d9f848b9",
   "metadata": {},
   "source": [
    "The dataset contains 768 observable values with eight feature variables and one target variable. Before starting to analyze the data and draw any conclusions, it is essential to understand the presence of missing values in any dataset. To do so, the simplest way is to use 'df.info()' function which will provide us the column names with the type of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60115f86-de7a-41c8-8522-f58ace5d5854",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get initial information on the dataset including data types and null values\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bf3143c-7229-418c-b5b0-f84e0a7ab835",
   "metadata": {},
   "source": [
    "There are five features in the data that contain null values, Glucose, BloodPressure, SkinThickness, Insulin and BMI. A null value is a special marker to indicate that a data value does not exist in the database. In other words, it is just a placeholder to denote values that are missing or that we do not know. </br> \n",
    "We can perform a quick calculation and print the percentage of missing values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989c735-2799-4b18-87af-43153a11dc1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Handling Missing Data\n",
    "# Identify columns with null values and print percentage of missing values for each\n",
    "# Function to calculate and display missing values\n",
    "def display_missing_data(dataframe):\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    percent_missing = (missing_values * 100) / dataframe.shape[0]\n",
    "    missing_df = pd.DataFrame({'Column': dataframe.columns,\n",
    "                                 'Total Missing Values': missing_values,\n",
    "                                 'Percent of Missing Values': percent_missing})\n",
    "    print(missing_df)\n",
    "\n",
    "print(\"\\nMissing Data:\")\n",
    "display_missing_data(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92549ad4-963b-4553-bf9c-4b5be48813fd",
   "metadata": {},
   "source": [
    "Next, we can try and discard rows containing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c9313-aeb5-432d-972c-3d10d3781233",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop all rows containing a null values and get information on the new dataset\n",
    "df.dropna().info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27667fce-e856-4147-ace8-efc72969f0d1",
   "metadata": {},
   "source": [
    "Removing all rows containing null values significantly reduced our dataset to only 392 entries. Instead, we can perform mean imputation, or mean substitution, replacing missing values of a certain variable by the mean of non-missing cases of that variable.\n",
    "\n",
    "Note: Mean imputation was chosen as it is a simple and common method of handling missing data. \n",
    "However, it can introduce bias if the data is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e784d33-0a4c-43cd-b2c4-315dea77d7e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handling missing data - Mean Imputation\n",
    "missing_cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "for col in missing_cols:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# Display information of the cleaned dataset\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32d20932-0e5c-40aa-bc02-b9b59a78cffe",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing and exploration\n",
    "### Data Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beb441b0-2c4f-4eb7-a99a-9bd880f27d09",
   "metadata": {},
   "source": [
    "To get an initial ‘feel’ of data we can plot a few visualization schemes for our data.\n",
    "It's essential to understand our data before we start with our analyses. Data visualization is a great way to identify trends, patterns, and relationships in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2765f1-0144-4bd7-a348-f46df1da49eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "# Boxplots help us identify outliers and understand our data's distribution.\n",
    "plt.figure(figsize=(18, 6), dpi=80)\n",
    "sns.boxplot(data=df, orient=\"h\", palette=\"Set1\")\n",
    "plt.title('Boxplot of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81e26e-828e-45ae-a3c7-d37029cbecd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Heatmap correlation\n",
    "# A heatmap is used to understand the correlation between different features in the dataset. \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181a736-864d-423c-80c8-d27d764163cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pairwise Relationships\n",
    "# It helps to visualize pairwise relationships between different features.\n",
    "# We'll use a PairGrid, which allows us to customize what kind of plots are shown.\n",
    "graph = sns.PairGrid(df, hue ='Outcome')\n",
    "\n",
    "# Histograms for the diagonal subplots\n",
    "graph = graph.map_diag(plt.hist)\n",
    "\n",
    "# Scatterplots for the non-diagonal subplots\n",
    "graph = graph.map_offdiag(plt.scatter)\n",
    "\n",
    "# Add legend\n",
    "graph = graph.add_legend()\n",
    "\n",
    "# Note: This operation may take a few seconds due to the amount of data being processed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb3e9a81-1a49-464f-80cb-2e2d8795fce6",
   "metadata": {},
   "source": [
    "Open discussion - what can we learn from the plots?\n",
    "\n",
    "1. What insights can we gather from the boxplots? Are there any features with a significant number of outliers?\n",
    "\n",
    "2. Looking at the heatmap, which features appear to be highly correlated with each other? What might be the reason for this correlation?\n",
    "\n",
    "3. In the pairwise relationship plots, do you notice any specific trends or patterns between different features? How could these relationships impact the outcome?\n",
    "\n",
    "4. Considering the data visualization results, do you have any preliminary hypotheses or predictions about which features might be most predictive of the outcome?\n",
    "\n",
    "5. How could we further enhance our data visualization to extract more insights? For example, could additional plot types or more complex visualizations provide further clarity?\n",
    "\n",
    "6. If we found that two features were strongly correlated, what might be the implications for our predictive modeling? Would it be beneficial to include both features in our model, or just one?\n",
    "\n",
    "7. Based on these visualizations, are there any features that seem less relevant to the outcome? Could we consider removing them from our future analyses to simplify our model?\n",
    "\n",
    "**Remember that these are open-ended questions and there might not be a single correct answer. The goal is to promote discussion and deeper analysis of the data.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccdb6d86-df53-48cf-bea3-4a91d81b2f1d",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146debeb-b941-4223-94bb-e5e9d20f0535",
   "metadata": {},
   "source": [
    "The next step in our process is to prepare our data for the classification training. This involves separating our dataset into two parts - features and targets. In data science, it's conventional to denote features with an 'X' and targets with 'y'. Let's start with this split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cff7b-93ab-4e17-ad3a-da562f59538b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a1773d2",
   "metadata": {},
   "source": [
    "To get a sense of our target variable distribution, we can plot a countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Outcome\", data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90455c73-cdbc-4170-b463-23e85d94fedb",
   "metadata": {},
   "source": [
    "Once we have our features and targets defined, we will partition our dataset into training and testing sets. A typical split might be 80% of the data for training, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4067e6-8610-4639-8cb3-e50786e77b88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c1c636d-2980-48f8-a4a5-40748d250c01",
   "metadata": {},
   "source": [
    "Before we implement our classification algorithm, we need to scale the feature variables of our dataset. This process, often called feature scaling, involves adjusting the distribution of each feature such that the mean of observed values is 0 and the standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b890-ccd0-4840-afe1-5f3e08241483",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_x = StandardScaler()\n",
    "X_train = scaling_x.fit_transform(X_train)\n",
    "X_test = scaling_x.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87523661",
   "metadata": {},
   "source": [
    "Why is feature scaling important?\n",
    "\n",
    "In many machine learning algorithms, the result can vary based on the scale of the features. This is especially true for algorithms that use a distance-based metric for calculations. If one feature has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19349a7c-4f6f-4d47-adfc-8848bfb3919b",
   "metadata": {},
   "source": [
    "### Training and Evaluating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165867d3-c6eb-455f-b401-c31d9d50704f",
   "metadata": {},
   "source": [
    "Now, we'll move on to the main part of this workshop, where we experiment with various prediction algorithms and assess their performance on our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130b201d-232e-46ee-a928-96307faa2fac",
   "metadata": {},
   "source": [
    "#### 3.1 K-Nearest  (KNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5d7e519-20ec-45fb-955e-72dbe8d6beb7",
   "metadata": {},
   "source": [
    "KNN is a non-parametric and lazy learning algorithm. Non-parametric means that there is no assumption for the underlying data distribution, allowing flexibility for datasets that don't adhere to specific mathematical assumptions. Lazy learning refers to the fact that the algorithm doesn't require any training data points for model generation; instead, all training data is used in the testing phase.\n",
    "\n",
    "In KNN, the value of K represents the number of nearest neighbors to consider. It is a crucial deciding factor, typically an odd number for binary classification problems. When K equals 1, the algorithm is known as the nearest neighbor algorithm, which is the simplest case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a9bf053",
   "metadata": {},
   "source": [
    "![title](../../2023/img/knn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc4bd432",
   "metadata": {},
   "source": [
    "Let's start by training a KNN model with just one neighbor and evaluate its classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebbbf1-b1c8-4209-ae54-e6efd4352f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN with one neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Calssifiction accuracy with one neighbor: \", \"{:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60fd5953-2a39-4646-822d-b0fd49bec5e1",
   "metadata": {},
   "source": [
    "With only one neighbor our model’s classification accuracy is 75.3%. </br>\n",
    "\n",
    "Next, we can explore a range of neighbor values to see if we can improve the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a634a-fe48-4f79-9279-7dee4e79e4d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "error1 = []\n",
    "error2 = []\n",
    "neighbors = range(1, 50)\n",
    "\n",
    "for k in neighbors:\n",
    "    # Using KNN algorithm\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred1 = knn.predict(X_train)\n",
    "\n",
    "    # Storing the errors\n",
    "    error1.append(np.mean(y_train != y_pred1))\n",
    "    y_pred2 = knn.predict(X_test)\n",
    "    error2.append(np.mean(y_test != y_pred2))\n",
    "\n",
    "# Plotting the error rates for training and testing\n",
    "plt.plot(neighbors, error1, label=\"Train\")\n",
    "plt.plot(neighbors, error2, label=\"Test\")\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "\n",
    "min_knn_value = min(error2)\n",
    "min_knn_index = error2.index(min_knn_value)\n",
    "print(\"Minimum error with {} neighbors: {:.3f}\".format(min_knn_index, min_knn_value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb7928e4-e052-47ea-a94d-289ee2ff060b",
   "metadata": {},
   "source": [
    "From the error rate plot, we can observe the minimum test error is achieved with 31 neighbors. We can compute the accuracy value for that k. Additionally, let's compute the confusion matrix to evaluate the precision of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855afce-935a-4cc5-847b-ca0041f9a5ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=31)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f26159",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics:\n",
    "The classification report provides us with various evaluation metrics such as precision, recall, and F1-score. Precision represents the ratio of true positive predictions to the total predicted positives, recall represents the ratio of true positive predictions to the total actual positives, and F1-score is the harmonic mean of precision and recall. These metrics give us a comprehensive understanding of the model's performance.\n",
    "\n",
    "##### Confusion Matrix:\n",
    "The confusion matrix visualizes the performance of our model by showing the true positive, true negative, false positive, and false negative predictions. It helps us understand the model's ability to correctly classify instances and detect any biases or imbalances in the predictions.\n",
    "\n",
    "By analyzing the classification report and the confusion matrix, we can gain valuable insights into the model's precision, recall, and overall performance.\n",
    "\n",
    "In summary, the KNN model with 31 neighbors achieves the minimum error rate on the test set. The evaluation metrics provide a comprehensive understanding of the model's performance, enabling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ba3218-7680-4952-acb0-0ac69b966542",
   "metadata": {},
   "source": [
    "#### 3.2 Support Vector Machines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5df12703-d000-4321-942a-a02c9894d2d1",
   "metadata": {},
   "source": [
    "Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76211ec9-f619-408c-900d-c4f035bb7bf2",
   "metadata": {},
   "source": [
    "![title](../../2023/img/svm.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef97843e",
   "metadata": {},
   "source": [
    "Let's train an SVM classifier with a linear kernel and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78795b22-c237-4966-a5ab-26f142525916",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a svm Classifier\n",
    "clf = SVC(kernel='linear') # Linear Kernel\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "\n",
    "# Classifiction report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f261bbf-787d-4373-932b-49a880f7c23e",
   "metadata": {},
   "source": [
    "#### 3.3 Random Forests Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93f2fc61-d02e-4832-bc86-45e191f9b004",
   "metadata": {},
   "source": [
    "Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd3305-a6a5-4d0a-89a9-b822368a3249",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "\n",
    "# Classifiction report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79ccb996-e5d1-4a77-9d66-93ce76989d8e",
   "metadata": {},
   "source": [
    "Random forests also offers a good feature selection indicator. The below compution plots the relative importance or contribution of each feature in the prediction. It automatically computes the relevance score of each feature in the training phase. Then it scales the relevance down so that the sum of all scores is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c5de3-c886-45e6-84b9-0300f603de01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "feature_imp = pd.Series(rfc.feature_importances_,index=df.columns[:-1]).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "542a64ee",
   "metadata": {},
   "source": [
    "#### 3.4 Neural Network Classifier\n",
    "Neural networks are a popular choice for various machine learning tasks, including classification. They consist of multiple interconnected layers of nodes (neurons) that process input data and learn to make predictions. For this example, we'll use the Keras library, which provides a high-level API for building and training neural networks.\n",
    "\n",
    "We can define and train a neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(8,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d33392f",
   "metadata": {},
   "source": [
    "After training the model, we can evaluate its performance and visualize the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbd9b4a",
   "metadata": {},
   "source": [
    "This neural network example provides an alternative approach to predicting diabetes based on the given dataset. By adjusting the architecture, hyperparameters, and training process, you can explore different variations and potentially improve the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f85f2c-3967-4cdd-922e-8054702f3ced",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b568c13-8462-4c31-ba61-f37df01044a0",
   "metadata": {},
   "source": [
    "1. Based on the performance evaluation of the different classification algorithms (K-Nearest Neighbors, Support Vector Machines, Random Forests, Neural Networks), which algorithm would you recommend using for predicting diabetes? What factors influenced your decision?\n",
    "\n",
    "2. In the feature importance plot generated by the Random Forests classifier, which features appear to have the highest predictive power for diabetes? How do these findings align with existing medical knowledge about diabetes risk factors?\n",
    "\n",
    "3. The neural network model achieved a certain level of accuracy on the test set. Can you think of any strategies to further improve the model's performance? How would you approach optimizing the neural network architecture or adjusting hyperparameters?\n",
    "\n",
    "4. How might the inclusion of additional data sources or features (such as patient demographics or lifestyle factors) impact the accuracy and utility of the prediction models? What challenges or opportunities might arise when integrating diverse data types?\n",
    "\n",
    "5. In the context of healthcare, what potential benefits and risks are associated with using machine learning models for disease prediction? How can we ensure that these models are trustworthy and transparent to both healthcare professionals and patients?\n",
    "\n",
    "6. Reflecting on the workshop as a whole, what were the most significant insights or takeaways you gained from exploring big data in healthcare and applying machine learning to predict diabetes? How might you apply this knowledge and experience in future healthcare data analysis projects?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65876ebe",
   "metadata": {},
   "source": [
    "#### A static version of this file can be found in my 'github' repository: [https://github.com/hadasvolk/8400ML-WS](https://github.com/hadasvolk/8400ML-WS)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
